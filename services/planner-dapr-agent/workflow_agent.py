"""
Multi-step agent workflow: Planning → Execution → Testing
Minimal implementation using OpenAI Agents SDK best practices.

This module provides a three-phase workflow:
1. Planning Phase - Agent loop that researches, thinks, and writes a plan
2. Execution Phase - Agent loop that executes the plan using tools
3. Testing Phase - Agent loop that verifies the implementation

Key design decisions:
- Structured Output for Loop Control: output_type with Pydantic models signals phase completion
- Minimal Tool Set: Only essential tools, each with clear purpose
- Code-Based Orchestration: Deterministic phase sequencing (not LLM-decided)
"""

import asyncio
import os
import subprocess
from pathlib import Path
from typing import List, Optional

from pydantic import BaseModel, Field
from agents import Agent, Runner, function_tool

# Default workspace directory
DEFAULT_CWD = os.getenv("PLANNER_CWD", "/app/workspace")


# ============================================================================
# Structured Output Types (Control Loop Termination)
# ============================================================================

class Task(BaseModel):
    """A single task in the plan - matches existing planner-dapr-agent format."""
    id: str
    subject: str
    description: str
    status: str = "pending"
    blockedBy: List[str] = Field(default_factory=list)  # Tasks that must complete first
    blocks: List[str] = Field(default_factory=list)      # Tasks blocked by this one


class TestCase(BaseModel):
    """A test case to verify task completion."""
    id: str
    task_id: str  # Which task this tests
    description: str
    test_type: str  # "command", "file_exists", "output_contains"
    command: Optional[str] = None  # For command-based tests
    expected: Optional[str] = None  # Expected output or file path


class Plan(BaseModel):
    """Structured plan output - terminates planning loop."""
    summary: str
    tasks: List[Task]
    tests: List[TestCase]  # Verification tests for each task
    reasoning: str


class ExecutionResult(BaseModel):
    """Execution result - terminates execution loop."""
    success: bool
    completed_tasks: List[str]
    output: str
    errors: List[str] = Field(default_factory=list)


class TestResult(BaseModel):
    """Test result - terminates testing loop."""
    passed: bool
    tests_run: int
    tests_passed: int
    tests_failed: int
    failures: List[str] = Field(default_factory=list)
    summary: str


# ============================================================================
# Planning Phase Tools
# ============================================================================

@function_tool
async def research(query: str) -> str:
    """Search codebase or documentation for information needed to plan.

    Args:
        query: Search term to look for in the codebase

    Returns:
        List of files containing the query or a no-match message
    """
    try:
        result = subprocess.run(
            ["grep", "-r", "-l", query, DEFAULT_CWD],
            capture_output=True, text=True, timeout=30
        )
        files = result.stdout.strip().split("\n")[:10]
        if files[0]:
            # Make paths relative for readability
            rel_files = [os.path.relpath(f, DEFAULT_CWD) for f in files if f]
            return f"Found {len(rel_files)} relevant files: {rel_files}"
        return "No matches found"
    except subprocess.TimeoutExpired:
        return "Search timed out after 30s"
    except Exception as e:
        return f"Search error: {e}"


@function_tool
async def think(thought: str) -> str:
    """Record a reasoning step. Use this to think through the problem.

    Args:
        thought: Your reasoning or analysis to record

    Returns:
        Acknowledgment that the thought was recorded
    """
    return f"Noted: {thought}"


@function_tool
async def draft_plan(summary: str, tasks_json: str) -> str:
    """Draft or refine the plan. Call multiple times to iterate.

    Args:
        summary: High-level summary of the plan
        tasks_json: JSON string containing list of task objects with id, subject, description, blockedBy

    Returns:
        Status message about the draft
    """
    import json
    try:
        tasks = json.loads(tasks_json)
    except json.JSONDecodeError as e:
        return f"Error: Invalid JSON in tasks_json: {e}"

    if not isinstance(tasks, list):
        return "Error: tasks_json must be a JSON array"

    # Validate task structure
    for i, task in enumerate(tasks):
        if not isinstance(task, dict):
            return f"Error: Task {i} is not an object"
        if not task.get("id"):
            return f"Error: Task {i} missing 'id' field"
        if not task.get("subject"):
            return f"Error: Task {i} missing 'subject' field"
        if not task.get("description"):
            return f"Error: Task {i} missing 'description' field"

    return f"Draft plan with {len(tasks)} tasks recorded. Continue refining or finalize by outputting the Plan."


# ============================================================================
# Execution Phase Tools
# ============================================================================

@function_tool
async def read_file(file_path: str) -> str:
    """Read contents of a file.

    Args:
        file_path: Path to file (relative to workspace or absolute)

    Returns:
        File contents or error message
    """
    # Handle both relative and absolute paths
    if os.path.isabs(file_path):
        full_path = file_path
    else:
        full_path = os.path.join(DEFAULT_CWD, file_path)

    try:
        with open(full_path, "r") as f:
            content = f.read()
            if len(content) > 10000:
                return content[:10000] + f"\n\n... (truncated, {len(content)} total bytes)"
            return content
    except FileNotFoundError:
        return f"Error: File not found: {file_path}"
    except Exception as e:
        return f"Error reading {file_path}: {e}"


@function_tool
async def write_file(file_path: str, content: str) -> str:
    """Write content to a file. Creates directories if needed.

    Args:
        file_path: Path to file (relative to workspace or absolute)
        content: Content to write

    Returns:
        Success message or error
    """
    # Handle both relative and absolute paths
    if os.path.isabs(file_path):
        full_path = file_path
    else:
        full_path = os.path.join(DEFAULT_CWD, file_path)

    try:
        Path(full_path).parent.mkdir(parents=True, exist_ok=True)
        with open(full_path, "w") as f:
            f.write(content)
        return f"Wrote {len(content)} bytes to {file_path}"
    except Exception as e:
        return f"Error writing {file_path}: {e}"


@function_tool
async def run_command(command: str) -> str:
    """Run a shell command. Use for builds, tests, etc.

    Args:
        command: Shell command to execute

    Returns:
        Command output or error message
    """
    try:
        result = subprocess.run(
            command, shell=True, capture_output=True, text=True,
            timeout=60, cwd=DEFAULT_CWD
        )
        output = result.stdout + result.stderr
        if not output:
            return f"Command completed with exit code {result.returncode}"
        if len(output) > 5000:
            return output[:5000] + f"\n\n... (truncated, {len(output)} total chars)"
        return output
    except subprocess.TimeoutExpired:
        return "Command timed out after 60s"
    except Exception as e:
        return f"Error: {e}"


@function_tool
async def mark_task_complete(task_id: str, notes: str = "") -> str:
    """Mark a task as complete. Call after finishing each task.

    Args:
        task_id: ID of the task to mark complete
        notes: Optional notes about what was done

    Returns:
        Confirmation message
    """
    return f"Task {task_id} marked complete. {notes}"


# ============================================================================
# Testing Phase Tools
# ============================================================================

@function_tool
async def run_tests(command: str) -> str:
    """Run test command (pytest, npm test, etc.).

    Args:
        command: Test command to execute

    Returns:
        Test output or error message
    """
    try:
        result = subprocess.run(
            command, shell=True, capture_output=True, text=True,
            timeout=120, cwd=DEFAULT_CWD
        )
        output = result.stdout + result.stderr
        if not output:
            return f"Tests completed with exit code {result.returncode}"
        if len(output) > 8000:
            return output[:8000] + f"\n\n... (truncated, {len(output)} total chars)"
        return output
    except subprocess.TimeoutExpired:
        return "Tests timed out after 120s"
    except Exception as e:
        return f"Error running tests: {e}"


@function_tool
async def verify_output(task_id: str, expected: str, actual: str) -> str:
    """Verify a task's output matches expectations.

    Args:
        task_id: ID of the task being verified
        expected: Expected substring in the output
        actual: Actual output to check

    Returns:
        PASS or FAIL message
    """
    matches = expected.strip() in actual.strip()
    if matches:
        return f"Task {task_id}: PASS - Expected '{expected[:100]}' found in output"
    return f"Task {task_id}: FAIL - Expected '{expected[:100]}' not found in output"


@function_tool
async def check_file_exists(file_path: str) -> str:
    """Check if a file was created/modified as expected.

    Args:
        file_path: Path to check (relative to workspace or absolute)

    Returns:
        PASS or FAIL message with file info
    """
    # Handle both relative and absolute paths
    if os.path.isabs(file_path):
        full_path = file_path
    else:
        full_path = os.path.join(DEFAULT_CWD, file_path)

    if os.path.exists(full_path):
        size = os.path.getsize(full_path)
        return f"PASS: {file_path} exists ({size} bytes)"
    return f"FAIL: {file_path} does not exist"


# ============================================================================
# Agent Definitions
# ============================================================================

def create_planning_agent(model: str = "gpt-5.2-codex") -> Agent:
    """Create the planning agent.

    Args:
        model: OpenAI model to use

    Returns:
        Configured planning Agent
    """
    return Agent(
        name="Planner",
        model=model,
        instructions="""You are a planning agent. Create a plan with tasks and test cases.

CRITICAL: For simple tasks, output the Plan IMMEDIATELY without using any tools.

TOOLS (optional, for complex tasks only):
- `research`: Search codebase for context (ONLY if you need to understand existing code)
- `think`: Record reasoning (ONLY if the task is complex)
- `draft_plan`: Draft iterations (ONLY if you need to refine)

OUTPUT (required):
Output a Plan with:
- summary: Brief description of what will be done
- tasks: List with id, subject, description, blockedBy (dependencies)
- tests: List with id, task_id, description, test_type ("command", "file_exists", or "output_contains"), command, expected
- reasoning: Brief explanation

RULES:
- Simple tasks (print, echo, hello world) → Output Plan immediately, no tools
- Complex tasks → Use 1-2 tool calls maximum, then output Plan
- Each task needs exactly one test case
- Use blockedBy for task dependencies""",
        tools=[research, think, draft_plan],
        output_type=Plan,  # Loop terminates when Plan is output
    )


def create_execution_agent(model: str = "gpt-5.2-codex") -> Agent:
    """Create the execution agent.

    Args:
        model: OpenAI model to use

    Returns:
        Configured execution Agent
    """
    return Agent(
        name="Executor",
        model=model,
        instructions="""You are an execution agent. Execute the plan you receive.

WORKFLOW:
1. Review the plan and tasks
2. Execute tasks in dependency order (respect blockedBy)
3. Use tools to complete each task
4. Call `mark_task_complete` after each task
5. Output ExecutionResult when all tasks are done

RULES:
- Follow the plan exactly
- Handle errors gracefully
- Report what was accomplished""",
        tools=[read_file, write_file, run_command, mark_task_complete],
        output_type=ExecutionResult,  # Loop terminates when Result is output
    )


def create_testing_agent(model: str = "gpt-5.2-codex") -> Agent:
    """Create the testing agent.

    Args:
        model: OpenAI model to use

    Returns:
        Configured testing Agent
    """
    return Agent(
        name="Tester",
        model=model,
        instructions="""You are a testing agent. Verify the implementation works.

WORKFLOW:
1. Review the test cases from the plan
2. Run each test using appropriate tools
3. If tests fail, report what went wrong
4. Output TestResult with pass/fail summary

RULES:
- Run ALL test cases from the plan
- Be thorough in verification
- Report specific failures for debugging""",
        tools=[run_tests, verify_output, check_file_exists, read_file, run_command],
        output_type=TestResult,  # Loop terminates when TestResult is output
    )


# ============================================================================
# Sandboxed Agent Factories (for Kubernetes Agent Sandbox execution)
# ============================================================================

def create_execution_agent_sandboxed(sandbox, model: str = "gpt-5.2-codex") -> Agent:
    """Create an execution agent with sandbox-aware tools.

    This factory creates an execution agent whose tools (run_command, write_file,
    read_file) execute in an isolated Kubernetes Agent Sandbox pod rather than
    the local filesystem.

    Args:
        sandbox: An active SandboxExecutor instance
        model: OpenAI model to use

    Returns:
        Configured execution Agent with sandbox tools
    """
    from sandbox_executor import create_sandboxed_tools

    sandboxed_tools = create_sandboxed_tools(sandbox)

    return Agent(
        name="Executor",
        model=model,
        instructions="""You are an execution agent. Execute the plan you receive.

WORKFLOW:
1. Review the plan and tasks
2. Execute tasks in dependency order (respect blockedBy)
3. Use tools to complete each task
4. Call `mark_task_complete` after each task
5. Output ExecutionResult when all tasks are done

RULES:
- Follow the plan exactly
- Handle errors gracefully
- Report what was accomplished

NOTE: You are running in an isolated sandbox environment. All file and command
operations are executed in this sandbox, which provides security isolation.""",
        tools=sandboxed_tools["execution"],
        output_type=ExecutionResult,
    )


def create_testing_agent_sandboxed(sandbox, model: str = "gpt-5.2-codex") -> Agent:
    """Create a testing agent with sandbox-aware tools.

    This factory creates a testing agent whose tools execute in the same
    Kubernetes Agent Sandbox pod as the execution agent, allowing it to
    verify files created during execution.

    Args:
        sandbox: An active SandboxExecutor instance (same as execution)
        model: OpenAI model to use

    Returns:
        Configured testing Agent with sandbox tools
    """
    from sandbox_executor import create_sandboxed_tools

    sandboxed_tools = create_sandboxed_tools(sandbox)

    return Agent(
        name="Tester",
        model=model,
        instructions="""You are a testing agent. Verify the implementation works.

WORKFLOW:
1. Review the test cases from the plan
2. Run each test using appropriate tools
3. If tests fail, report what went wrong
4. Output TestResult with pass/fail summary

RULES:
- Run ALL test cases from the plan
- Be thorough in verification
- Report specific failures for debugging

NOTE: You are running in the same isolated sandbox environment as the execution
agent. Files created during execution are available for testing.""",
        tools=sandboxed_tools["testing"],
        output_type=TestResult,
    )


# ============================================================================
# Workflow Orchestration
# ============================================================================

async def run_workflow(
    prompt: str,
    model: str = "gpt-5.2-codex",
    max_turns: int = 20,
    max_test_retries: int = 3
) -> dict:
    """
    Run the complete planning → execution → testing workflow.

    Args:
        prompt: User's task description
        model: OpenAI model to use
        max_turns: Max iterations per phase (safety limit)
        max_test_retries: Max times to retry execution if tests fail

    Returns:
        dict with plan, execution, and test results
    """
    # Phase 1: Planning
    planning_agent = create_planning_agent(model)
    plan_result = await Runner.run(
        planning_agent,
        input=prompt,
        max_turns=max_turns,
    )
    plan: Plan = plan_result.final_output

    # Auto-populate blocks based on blockedBy
    task_map = {t.id: t for t in plan.tasks}
    for task in plan.tasks:
        for blocked_by_id in task.blockedBy:
            if blocked_by_id in task_map:
                if task.id not in task_map[blocked_by_id].blocks:
                    task_map[blocked_by_id].blocks.append(task.id)

    # Phase 2: Execution
    execution_agent = create_execution_agent(model)
    exec_prompt = f"""Execute this plan:

Summary: {plan.summary}

Tasks:
{chr(10).join(f"- [{t.id}] {t.subject}: {t.description} (blockedBy: {t.blockedBy})" for t in plan.tasks)}

Reasoning: {plan.reasoning}"""

    exec_result = await Runner.run(
        execution_agent,
        input=exec_prompt,
        max_turns=max_turns,
    )
    execution: ExecutionResult = exec_result.final_output

    # Phase 3: Testing (loops until pass or max retries)
    testing_agent = create_testing_agent(model)
    test_prompt = f"""Verify the implementation:

Plan Summary: {plan.summary}

Test Cases:
{chr(10).join(f"- [{tc.id}] {tc.description} (type: {tc.test_type}, command: {tc.command})" for tc in plan.tests)}

Execution Summary: {execution.output}
Completed Tasks: {execution.completed_tasks}"""

    test: TestResult = TestResult(
        passed=False, tests_run=0, tests_passed=0, tests_failed=0,
        failures=[], summary="Tests not yet run"
    )

    for attempt in range(max_test_retries):
        test_result = await Runner.run(
            testing_agent,
            input=test_prompt,
            max_turns=max_turns,
        )
        test = test_result.final_output

        if test.passed:
            break  # All tests passed!

        # Tests failed - could add re-execution logic here
        # For now, just report the failure

    return {
        "plan": plan.model_dump(),
        "execution": execution.model_dump(),
        "testing": test.model_dump(),
        "status": "completed" if test.passed else "failed",
    }


# ============================================================================
# CLI Entry Point
# ============================================================================

if __name__ == "__main__":
    import sys
    import json

    prompt = sys.argv[1] if len(sys.argv) > 1 else "Create a hello world Python script"

    print(f"Running workflow for: {prompt}")
    print("=" * 60)

    result = asyncio.run(run_workflow(prompt))

    print("\n=== PLAN ===")
    print(json.dumps(result["plan"], indent=2))
    print("\n=== EXECUTION ===")
    print(json.dumps(result["execution"], indent=2))
    print("\n=== TESTING ===")
    print(json.dumps(result["testing"], indent=2))
    print("\n=== STATUS ===")
    print(f"Workflow {'PASSED' if result['status'] == 'completed' else 'FAILED'}")
